{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from coarsen import *\n",
    "from scipy import io\n",
    "from torch_geometric.transforms import FaceToEdge\n",
    "from models_gpu import *\n",
    "from visualize import *\n",
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_graph(mat,index):\n",
    "    '''\n",
    "    get_graph: Reads a single data point from already-loaded matlab data\n",
    "    \n",
    "    mat - The dictionary of values read from a .mat file\n",
    "    index - The index of the data point\n",
    "    \n",
    "    Returns - The Data() representation of 'mat'\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    nodes = mat['nodes'][index,0].T\n",
    "    elems = mat['elem'][index,0].T-1\n",
    "    stress = mat['stress'][index,0]\n",
    "    dt = mat['dt'][index,0]\n",
    "    sdf = mat['sdf'][index][0].T\n",
    "\n",
    "    f2e = FaceToEdge(remove_faces=True)\n",
    "\n",
    "    x = torch.tensor(np.concatenate((nodes,dt), axis=1), dtype=torch.float)\n",
    "    y = torch.tensor(stress, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, face = torch.tensor(elems.T), y=y)\n",
    "    data = f2e(data)\n",
    "    data.sdf = torch.tensor(sdf, dtype=torch.float)\n",
    "    return data\n",
    "\n",
    "def load_matlab_dataset(filename, scale = 10000):\n",
    "    '''\n",
    "    load_matlab_dataset: Loads a scalar field dataset from a .mat file  \n",
    "    \n",
    "    Inputs:\n",
    "    - filename - The .mat dataset consisting of meshes, the scalar field and SDF at each node, and an SDF array\n",
    "    - scale - The number to divide each scalar field value by, defaults to 10000   \n",
    "    \n",
    "    Returns:\n",
    "    - The dataset as a list of Data() objects\n",
    "    \n",
    "    '''\n",
    "    mat = io.loadmat(filename)\n",
    "    dataset = []\n",
    "    for i in range(len(mat['nodes'])):\n",
    "        data = get_graph(mat, i)\n",
    "        data.y /= scale\n",
    "        data.sdf = data.sdf[None, None, :, :] * 10\n",
    "        dataset.append(data)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "class StressDataset(Dataset):\n",
    "    def __init__(self, zipfiles, **kwargs):\n",
    "        self.zipfiles = zipfiles\n",
    "        self.options = kwargs\n",
    "        self.data = []\n",
    "        for zipfile in zipfiles:\n",
    "            self.data += load_matlab_dataset(zipfile)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = coarsen_data(self.data[idx], **self.options)\n",
    "        return X\n",
    "    \n",
    "def split_dataset(dataset, train_frac=0.8, seed=216):\n",
    "    train_size = int(0.8*len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size], generator)\n",
    "    print(f\"Splitting dataset with seed {seed}: {train_size} training, {test_size} testing\")\n",
    "    return train_dataset, test_dataset\n",
    "    \n",
    "class StressDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        super().__init__(dataset, collate_fn=merge_coarsened_data, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StressDataset([\"data/stress_vor_w.mat\", \"data/stress_lat_w.mat\"])\n",
    "dataset_tr, dataset_te = split_dataset(dataset)\n",
    "train_loader = StressDataLoader(dataset_tr, batch_size=4)\n",
    "test_loader = StressDataLoader(dataset_te, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_info = get_default_conv_info(\"EdgeConv\")\n",
    "conv_info[\"dims\"] = [64,64]\n",
    "model = GraphUNet(num_layers=3, num_channels=64, conv_info=conv_info, mlp_dims=[128,128,128]).to(device)\n",
    "\n",
    "\n",
    "# model = JustConvNet(num_convs=4, conv_dims=[64,64], channels=64, mlp_dims = [128,128,128]).to(device)\n",
    "\n",
    "\n",
    "# conv_info = get_default_conv_info(\"GCNConv\")\n",
    "# model = GraphUNet(num_layers=3, num_channels=64, conv_info=conv_info, mlp_dims=[128,128,128]).to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "epochs = 50\n",
    "opt = optim.Adam(params=model.parameters(), lr=lr)\n",
    "lossfun = nn.MSELoss().to(device)\n",
    "all_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    losses = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        loss = lossfun(model(data, device).reshape(-1,1), data.y.reshape(-1,1).to(device))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "        print(f\"Batch {i+1}/{len(train_loader)}: Loss = {loss.item():.6e}, Avg = {np.mean(np.array(losses)):.6e}          \",end=\"\\r\")\n",
    "        # del data\n",
    "        # torch.cuda.empty_cache() \n",
    "\n",
    "    if 1: # 0 == (epoch%5):\n",
    "        print(f\"Epoch: {epoch}/{epochs}, Loss: {np.mean(np.array(losses)):.6e}                             \")# (max {np.max(np.array(losses)):.6e}, median {np.median(np.array(losses)):.6e})                \")\n",
    "    all_losses.append(np.array(losses))\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "        loss = lossfun(model(data, device).reshape(-1,1), data.y.reshape(-1,1).to(device))\n",
    "        losses.append(loss.item())\n",
    "        # del data\n",
    "        # torch.cuda.empty_cache() \n",
    "    if 1: # 0 == (epoch%5):\n",
    "        print(f\"   Val. Loss: {np.mean(np.array(losses)):.6e}                              \")\n",
    "    val_losses.append(np.array(losses))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.savez(\"stress-model-losses.npz\", all_losses=np.array(all_losses), val_losses=np.array(val_losses))\n",
    "torch.save(model, \"stress-model.pth\")\n",
    "model = torch.load(\"stress-model.pth\").to(device)\n",
    "\n",
    "\n",
    "# np.savez(\"stress-model-plain-losses.npz\", all_losses=np.array(all_losses), val_losses=np.array(val_losses))\n",
    "# torch.save(model, \"stress-model-plain.pth\")\n",
    "# model = torch.load(\"stress-model-plain.pth\").to(device)\n",
    "\n",
    "\n",
    "# np.savez(\"stress-model-gcn-losses.npz\", all_losses=np.array(all_losses), val_losses=np.array(val_losses))\n",
    "# torch.save(model, \"stress-model-gcn.pth\")\n",
    "# model = torch.load(\"stress-model-gcn.pth\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = eval_model_multiple(model, dict(tr=dataset_tr, te=dataset_te), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"Performance:\")\n",
    "print(\"                    Median R2\")\n",
    "print(\"        Train set:  \", np.round(np.median(vals[\"tr\"]),4))\n",
    "print(\"         Test set:  \", np.round(np.median(vals[\"te\"]),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxes(vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 107\n",
    "data = dataset_te[i]\n",
    "plot_comparison(model, data, device=device)\n",
    "\n",
    "i = 105\n",
    "data = dataset_te[i]\n",
    "plot_comparison(model, data, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = torch.load(\"stress-model-plain.pth\").to(device)\n",
    "# model2 = torch.load(\"stress-model-gcn.pth\").to(device)\n",
    "# model3 = torch.load(\"stress-model.pth\").to(device)\n",
    "# models = dict(model1=model1, model2=model2, model3=model3)\n",
    "# model_names=dict(model1=\"Plain GNN (EdgeConv)\", model2=\"TAG U-Net (GCNConv)\", model3=\"TAG U-Net (EdgeConv)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2file=\"r2_data.npz\"\n",
    "# if os.path.isfile(r2file):\n",
    "#     r2s = np.load(r2file)\n",
    "# else:\n",
    "#     r2s = dict()\n",
    "#     for key in models:\n",
    "#         vals = eval_model_multiple(models[key], dict(tr=dataset_tr, te=dataset_te), device=device)\n",
    "#         r2s[key] = vals\n",
    "#     np.savez(r2file,r2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_color=dict(model1=\"darkred\", model2=\"blue\", model3=\"green\")\n",
    "# plt.figure(dpi=200)\n",
    "# for key in models:\n",
    "#     te = r2s[key][\"te\"]\n",
    "#     log = True\n",
    "#     bins = 20\n",
    "#     plt.hist(te[te>-1], bins=bins, density=True, histtype=\"step\", lw=3., edgecolor=model_color[key], label=model_names[key])\n",
    "#     #plt.hist(te[te>-1], bins=bins, density=True, alpha=0.3, , log=log, histtype=\"stepfilled\", color=model_color[key])\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"$R^2$\")\n",
    "# plt.ylabel(\"Probability Density\")\n",
    "# plt.title(\"2-D Stress Prediction Task\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_model_comparison(models, data, model_names, filename=None, dpi=300, size=13, device=\"cpu\"):\n",
    "#     N = len(models)\n",
    "#     plt.figure(figsize=(4.2*(N+1), 9.5), dpi=dpi)\n",
    "#     s = size# / (1 + 7*(3000<data.x.shape[0]))\n",
    "#     small_axes = []\n",
    "\n",
    "#     maxval = np.max(data.y.detach().numpy())\n",
    "#     for key in models:\n",
    "#         pred = models[key].to(device)(data, device=device).cpu().detach().numpy()\n",
    "#         maxval = max(maxval, np.max(pred))\n",
    "\n",
    "#     plt.subplot(2,N+1,1)\n",
    "#     plt.title(\"Ground Truth\", fontsize=17)\n",
    "#     plot_data(data, data.y, size=s, color_bounds=[0,maxval])\n",
    "    \n",
    "#     i = 2\n",
    "#     for key in models:\n",
    "#         model = models[key].to(device)\n",
    "#         pred = model(data, device=device).cpu().flatten()\n",
    "\n",
    "#         plt.subplot(2,N+1,i)\n",
    "#         plt.title(model_names[key], fontsize=17)\n",
    "#         plot_data(data, pred, size=s, color_bounds=[0,maxval])\n",
    "\n",
    "#         ax = plt.subplot(2,N+1,i+4)\n",
    "#         plot_model_r2(model, data, device=device)\n",
    "#         plt.axis(\"scaled\")\n",
    "#         small_axes.append(ax)\n",
    "#         i += 1\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.14, hspace=0)\n",
    "#     for ax in small_axes:\n",
    "#         pos1 = ax.get_position() # get the original position \n",
    "#         pos2 = [pos1.x0 + 0.01, pos1.y0 + 0.1/N,  pos1.width * 0.9, pos1.height * 0.9] \n",
    "#         ax.set_position(pos2) # set a new position\n",
    "\n",
    "#     if filename is not None:\n",
    "#         plt.savefig(filename, bbox_inches = \"tight\")\n",
    "#         plt.close()\n",
    "#     else:\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(dpi=300)\n",
    "# plt.imshow(np.flipud(dataset_te[imed].data[0].sdf.squeeze().detach().numpy()),cmap=\"seismic\", vmin=-2,vmax=2)\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.argsort(r2s[\"model3\"][\"te\"])\n",
    "# imed = idx[3*len(idx)//4-10]\n",
    "# plot_model_comparison(models, dataset_te[imed],model_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion-360-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
