{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import json\n",
    "\n",
    "# def copy_files_from_names(file_names, source_folder, dest_folder):\n",
    "#     for file in file_names:\n",
    "#         name = file+'.npz'\n",
    "#         source_file_path = os.path.join(source_folder, name)\n",
    "#         if os.path.exists(source_file_path):\n",
    "#             dest_file_path = os.path.join(dest_folder, name)\n",
    "#             shutil.copy(source_file_path, dest_file_path)\n",
    "\n",
    "# def copy_first_n_files(source_folder, dest_folder, n):\n",
    "#     files = os.listdir(source_folder)\n",
    "#     sorted_files = sorted(files)\n",
    "#     for file_name in sorted_files[:n]:\n",
    "#         source_file_path = os.path.join(source_folder, file_name)\n",
    "#         dest_file_path = os.path.join(dest_folder, file_name)\n",
    "#         shutil.copy(source_file_path, dest_file_path)\n",
    "# copy_first_n_files(\"data\", \"data-10000\",10000)\n",
    "\n",
    "# with open(\"recoat_train_val_test.json\") as f:\n",
    "#     j = json.load(f)\n",
    "\n",
    "# for key in j:\n",
    "#     copy_files_from_names(j[key], \"data\", f\"data-{key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_max_displacement_in_folder(folder):\n",
    "    files = os.listdir(folder)\n",
    "    maxes = []\n",
    "    bad_idx = []\n",
    "    for i, file in enumerate(files):\n",
    "        name = os.path.join(folder, file)\n",
    "        data = np.load(name)\n",
    "        maxval = np.max(np.abs(data[\"disp\"][:,2]))\n",
    "        maxes.append(maxval)\n",
    "        print(i, end=\"\\r\")\n",
    "        if maxval > 0.5:\n",
    "            bad_idx.append(i)\n",
    "    maxes = np.array(maxes)\n",
    "    return maxes, bad_idx\n",
    "    \n",
    "maxes, bad = get_max_displacement_in_folder(\"../data-train\")\n",
    "maxes_te, bad_te = get_max_displacement_in_folder(\"../data-test\")\n",
    "maxes_val, bad_val = get_max_displacement_in_folder(\"../data-val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4,3),dpi=300)\n",
    "plt.hist(maxes[maxes<.5],100)\n",
    "plt.xlabel(\"Maximum z-Displacement Magnitude\")\n",
    "plt.ylabel(\"Number of shapes\")\n",
    "plt.title(\"Displacement < 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import importlib\n",
    "from models_gpu import * # Modified\n",
    "from coarsen import *\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "import visualize\n",
    "from visualize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_graphs\n",
    "# import parse_netfabb_results\n",
    "import evaluate\n",
    "importlib.reload(load_graphs)\n",
    "# importlib.reload(parse_netfabb_results)\n",
    "importlib.reload(evaluate)\n",
    "from load_graphs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxes = []\n",
    "# bad_idx = []\n",
    "# for i, data in enumerate(train_dataset):\n",
    "#     maxval = torch.max(torch.abs(data.y))\n",
    "#     maxes.append(maxval)\n",
    "#     print(i, end=\"\\r\")\n",
    "#     if maxval > 1:\n",
    "#         bad_idx.append(i)\n",
    "# maxes = np.array(maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NetFabbDataset(\"../data-train\", npz_mode=True, N=3, factor=8, replace_edges=True)\n",
    "test_dataset = NetFabbDataset(\"../data-test\", npz_mode=True, N=3, factor=8, replace_edges=True)\n",
    "val_dataset = NetFabbDataset(\"../data-val\", npz_mode=True, N=3, factor=8, replace_edges=True)\n",
    "\n",
    "\n",
    "bad_names = [train_dataset.files[i] for i in bad]\n",
    "bad_names_te = [test_dataset.files[i] for i in bad_te]\n",
    "bad_names_val = [val_dataset.files[i] for i in bad_val]\n",
    "\n",
    "for name in bad_names:\n",
    "    train_dataset.files.remove(name)\n",
    "for name in bad_names_te:\n",
    "    test_dataset.files.remove(name)\n",
    "for name in bad_names_val:\n",
    "    val_dataset.files.remove(name)\n",
    "\n",
    "\n",
    "train_loader = NetFabbDataLoader(train_dataset, shuffle=True, batch_size=4)\n",
    "test_loader = NetFabbDataLoader(test_dataset, shuffle=True, batch_size=4)\n",
    "val_loader = NetFabbDataLoader(val_dataset, shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_info = get_default_conv_info(\"EdgeConv\")\n",
    "conv_info[\"dims\"] = [128,128]\n",
    "model = GraphUNet(num_layers=3, num_channels=128, conv_info=conv_info, mlp_dims=[256,256,256]).to(device)\n",
    "\n",
    "\n",
    "# conv_info = get_default_conv_info(\"GCNConv\")\n",
    "# model = GraphUNet(num_layers=3, num_channels=128, conv_info=conv_info, mlp_dims=[256,256,256]).to(device)\n",
    "\n",
    "\n",
    "# model = JustConvNet(num_convs=6, conv_dims=[128,128], channels=128, mlp_dims = [256,256,256]).to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 50\n",
    "opt = optim.Adam(params=model.parameters(), lr=lr)\n",
    "lossfun = nn.MSELoss().to(device)\n",
    "all_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    losses = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        loss = lossfun(model(data, device).reshape(-1,1), data.y.reshape(-1,1).to(device))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "        print(f\"Batch {i+1}/{len(train_loader)}: Loss = {loss.item():.6e}, Avg = {np.mean(np.array(losses)):.6e}          \",end=\"\\r\")\n",
    "        del data\n",
    "        torch.cuda.empty_cache() \n",
    "\n",
    "    if 1: # 0 == (epoch%5):\n",
    "        print(f\"Epoch: {epoch}/{epochs}, Loss: {np.mean(np.array(losses)):.6e}                             \")# (max {np.max(np.array(losses)):.6e}, median {np.median(np.array(losses)):.6e})                \")\n",
    "    all_losses.append(np.array(losses))\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "        loss = lossfun(model(data, device).reshape(-1,1), data.y.reshape(-1,1).to(device))\n",
    "        losses.append(loss.item())\n",
    "        del data\n",
    "        torch.cuda.empty_cache() \n",
    "    if 1: # 0 == (epoch%5):\n",
    "        print(f\"   Val. Loss: {np.mean(np.array(losses)):.6e}                              \")\n",
    "    val_losses.append(np.array(losses))\n",
    "    model.train()\n",
    "\n",
    "    np.savez(\"mdl-new-losses.npz\", all_losses=np.array(all_losses), val_losses=np.array(val_losses))\n",
    "    torch.save(model, \"mdl-new.pth\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(\"model-gnn-600k-f8-50-v2-losses.npz\", all_losses=np.array(all_losses), val_losses=np.array(val_losses))\n",
    "# torch.save(model, \"model-gnn-600k-f8-50-v2.pth\")\n",
    "#model = torch.load(\"../model-300k-25.pth\").to(device)\n",
    "\n",
    "# np.savez(\"unet-new-losses.npz\", all_losses=np.array(all_losses), val_losses=np.array(val_losses))\n",
    "# torch.save(model, \"unet-new.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = train_dataset[2]\n",
    "data = test_dataset[70]\n",
    "i = -1#len(test_dataset)//2+3\n",
    "\n",
    "#data = test_dataset[idx_te[i]]\n",
    "#print(vals[\"te\"][idx_te[i]])\n",
    "\n",
    "# i = 0\n",
    "# data = train_dataset[idx_tr[i]]\n",
    "# print(vals[\"tr\"][idx_tr[i]])\n",
    "\n",
    "verts = data.x.detach().numpy()\n",
    "gt = data.y.detach().numpy()\n",
    "pred = model(data, device).cpu().detach().numpy()\n",
    "error = pred.flatten() - gt.flatten()\n",
    "\n",
    "plot_fields(verts, [gt, pred, error], [\"Ground Truth\", \"Prediction\", \"Pred - GT\"], cmap=\"jet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = JustConvNet(num_convs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 50\n",
    "opt = optim.Adam(params=model2.parameters(), lr=lr)\n",
    "lossfun = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    losses = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        loss = lossfun(model2(data).reshape(-1,1), data.y.reshape(-1,1))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "        print(f\"Shape {i+1}: Loss = {loss.item():.6e}\",end=\"\\r\")\n",
    "    if 1: # 0 == (epoch%5):\n",
    "        print(f\"Epoch: {epoch}/{epochs}, Loss: {np.mean(np.array(losses)):.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = train_dataset[2]\n",
    "data = test_dataset[0]\n",
    "\n",
    "verts = data.x.detach().numpy()\n",
    "gt = data.y.detach().numpy()\n",
    "pred = model2(data).detach().numpy()\n",
    "error = pred.flatten() - gt.flatten()\n",
    "\n",
    "plot_fields(verts, [gt, pred, error], [\"Ground Truth\", \"Prediction\", \"Pred - GT\"], cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(evaluate)\n",
    "import evaluate\n",
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = dict(tr=train_dataset, te=test_dataset)\n",
    "titles = dict(tr=\"Training\",te=\"Testing\")\n",
    "\n",
    "vals = eval_model_multiple(model, dsets, device)\n",
    "plot_boxes(vals,titles, lims=[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tr = np.argsort(vals[\"tr\"])\n",
    "idx_te = np.argsort(vals[\"te\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2 = eval_model_multiple(model2, dsets)\n",
    "plot_boxes(vals2,titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(vals[\"tr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(vals[\"te\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = train_dataset[2]\n",
    "for i in range(40,48):\n",
    "    data = test_dataset[i]\n",
    "\n",
    "    verts = data.x.detach().numpy()\n",
    "    gt = data.y.detach().numpy()\n",
    "    pred = model(data).detach().numpy()\n",
    "    error = pred.flatten() - gt.flatten()\n",
    "\n",
    "    plot_fields(verts, [gt, pred, error], [\"Ground Truth\", \"Prediction\", \"Pred - GT\"], cmap=\"jet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
